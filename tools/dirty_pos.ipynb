{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from textblob import TextBlob\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, Reshape, GlobalAveragePooling1D, merge, Flatten, Bidirectional, CuDNNGRU, add, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'Dataset/'\n",
    "EMBEDDING_FILE='features/fast-text-300.txt'\n",
    "#EMBEDDING_FILE='features/glove.twitter.27B.200d.txt'\n",
    "#EMBEDDING_FILE='features/glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE=path + 'train.csv'\n",
    "TEST_DATA_FILE=path + 'test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 350\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')\n",
    "\n",
    "#Glove Vectors\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE, 'r', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    try:\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(\"Err on \", values[:3])\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Load the cleaned words\n",
    "########################################\n",
    "\n",
    "cl_path = 'features/cleanwords.txt'\n",
    "clean_word_dict = {}\n",
    "with open(cl_path, 'r', encoding='utf-8') as cl:\n",
    "    for line in cl:\n",
    "        print(line)\n",
    "        line = line.strip('\\n')\n",
    "        typo, correct = line.split(',')\n",
    "        clean_word_dict[typo] = correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "from collections import defaultdict\n",
    "# Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^?!.,:a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "# regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "word_count_dict = defaultdict(int)\n",
    "import re\n",
    "toxic_dict = {}\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    # dirty words\n",
    "    text = re.sub(r\"”\", \"\", text)\n",
    "    text = re.sub(r\"“\", \"\", text)\n",
    "    text = replace_numbers.sub(' ', text)\n",
    "\n",
    "    if count_null_words:\n",
    "        text = text.split()\n",
    "        for t in text:\n",
    "            word_count_dict[t] += 1\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "\n",
    "    return (text)\n",
    "\n",
    "list_sentences_train = train_df[\"comment_text_clean\"].fillna(\"no comment\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "list_sentences_test = test_df[\"comment_text_clean\"].fillna(\"no comment\").values\n",
    "\n",
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "#tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sent2pos(sentence):\n",
    "    try:\n",
    "        tag = TextBlob(sentence).tags\n",
    "    except:\n",
    "        print(sentence)\n",
    "        print(' '.join([word_index[word] for word in text]))\n",
    "\n",
    "    updated_sentence = ' '.join([i[0] for i in tag])\n",
    "    tagged = ' '.join([i[1] for i in tag])\n",
    "#     print(len(updated_sentence.split(' ')),len(text2.split(' ')))\n",
    "#     print(updated_sentence)\n",
    "#     print(tagged)\n",
    "    return updated_sentence, tagged\n",
    "    \n",
    "inverse_word_index = {v: k for k, v in word_index.items()}\n",
    "\n",
    "Pos_comments = []\n",
    "Pos_updated_sentence = []\n",
    "for text in sequences:\n",
    "    text1 = ' '.join([inverse_word_index[word] for word in text])\n",
    "    if not isinstance(text1, str):\n",
    "        print(text)\n",
    "        print(text1)\n",
    "    updated_sentence, text2 = sent2pos(text1)\n",
    "    Pos_updated_sentence.append(updated_sentence)\n",
    "    Pos_comments.append(text2)\n",
    "    assert len(updated_sentence.split(' ')) == len(text2.split(' ')), \"T1 {} T2 {} \".format(len(text), len(text2.split()))\n",
    "    \n",
    "Pos_test_comments = []\n",
    "Pos_test_updated_sentence = []\n",
    "for text in test_sequences:\n",
    "    text1 = ' '.join([inverse_word_index[word] for word in text])\n",
    "    updated_sentence, text2 = sent2pos(text1)\n",
    "    Pos_test_updated_sentence.append(updated_sentence)\n",
    "    Pos_test_comments.append(text2)\n",
    "    assert len(updated_sentence.split(' ')) == len(text2.split(' ')), \"T1 {} T2 {} \".format(len(text), len(text2.split()))\n",
    "\n",
    "    \n",
    "pos_tokenizer = Tokenizer(num_words=50, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "#tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "\n",
    "pos_tokenizer.fit_on_texts(Pos_comments + Pos_test_comments)\n",
    "\n",
    "sequences = pos_tokenizer.texts_to_sequences(Pos_comments)\n",
    "test_sequences = pos_tokenizer.texts_to_sequences(Pos_test_comments)\n",
    "\n",
    "pos_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(pos_word_index))\n",
    "\n",
    "pos_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', pos_data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "pos_test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', pos_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second valid \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for text in Pos_updated_sentence:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in Pos_test_updated_sentence:\n",
    "    test_comments.append(text_to_wordlist(text))\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "# tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "null_words = open('null-word.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) +'\\n')\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "#24146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('cleaned_text.txt', 'w', encoding='utf-8')\n",
    "for line in test_comments:\n",
    "    f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort null word\n",
    "null_count = {}\n",
    "with open('null-word.txt', 'r', encoding='utf-8') as nullword:\n",
    "    for line in nullword:\n",
    "        w, c = line.strip('\\n').split(', ')\n",
    "        null_count[w] = int(c)\n",
    "\n",
    "null_count = sorted(word_count_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "with open('null-word.txt', 'w', encoding='utf-8') as output:\n",
    "    for w, c in null_count:\n",
    "        output.write(w + \", \" + str(c) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "STAMP = 'pavel_rnn_%.2f_%.2f'%(0.5,0.5)\n",
    "\n",
    "def _train_model_by_auc(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    best_auc = -1\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    current_epoch = 1\n",
    "\n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epochs=1, validation_data=[val_x, val_y])\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "        current_auc = roc_auc_score(val_y, y_pred)\n",
    "        print(\"Epoch {} auc {:.6f} best_auc {:.6f}\".format(current_epoch, current_auc, best_auc))\n",
    "        current_epoch += 1\n",
    "        if best_auc < current_auc or best_auc == -1:\n",
    "            best_auc = current_auc\n",
    "            best_weights = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == 5:\n",
    "                break\n",
    "\n",
    "    model.set_weights(best_weights)\n",
    "    return model, best_auc\n",
    "\n",
    "def _train_model_by_logloss(model, batch_size, train_x, pos_train_x, train_y, val_x, pos_val_x, val_y, fold_id):\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=7)\n",
    "    bst_model_path = STAMP + str(fold_id) + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    train_data = {'Onehot':train_x, 'POS':pos_train_x}\n",
    "    val_data = {'Onehot':val_x, 'POS':pos_val_x}\n",
    "    hist = model.fit(train_data, train_y,\n",
    "        validation_data=(val_data, val_y),\n",
    "        epochs=50, batch_size=batch_size, shuffle=True,\n",
    "        callbacks=[early_stopping, model_checkpoint])\n",
    "    bst_val_score = min(hist.history['val_loss'])\n",
    "    predictions = model.predict(val_data)\n",
    "    auc = roc_auc_score(val_y, predictions)\n",
    "    print(\"AUC Score\", auc)\n",
    "    return model, bst_val_score, auc, predictions\n",
    "\n",
    "def train_folds(X, pos_x, y, fold_count, batch_size, get_model_func):\n",
    "    fold_size = len(X) // fold_count\n",
    "    models = []\n",
    "    fold_predictions = []\n",
    "    score = 0\n",
    "    total_auc = 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(X)\n",
    "\n",
    "        train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "        \n",
    "        val_x = X[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "        \n",
    "        pos_train_x = np.concatenate([pos_x[:fold_start], pos_x[fold_end:]])\n",
    "        \n",
    "        pos_val_x = pos_x[fold_start:fold_end]\n",
    "    \n",
    "        print(\"In fold #\", fold_id)\n",
    "        model, bst_val_score, auc, fold_prediction = _train_model_by_logloss(get_model_func(), batch_size, train_x,\n",
    "                                                                             pos_train_x, train_y, val_x, pos_val_x, \n",
    "                                                                             val_y, fold_id)\n",
    "        score += bst_val_score\n",
    "        total_auc += auc\n",
    "        fold_predictions.append(fold_prediction)\n",
    "        models.append(model)\n",
    "    return models, score / fold_count, total_auc / fold_count, fold_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.layers import Reshape\n",
    "adam_optimizer = optimizers.Adam(lr=1e-3 ** 64/256, decay=1e-8)\n",
    "\n",
    "def get_av_pos_cnn():\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)\n",
    "\n",
    "    filter_nums = 325 # 500->375, 400->373, 300->\n",
    "    drop = 0.5\n",
    "    dr_rate = 0.5\n",
    "    \n",
    "    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Onehot')\n",
    "    input_layer_2 = Input(shape=(MAX_SEQUENCE_LENGTH,), name='POS')\n",
    "    \n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)(input_layer)\n",
    "    \n",
    "    embedding_layer2 = Embedding(50,\n",
    "            30, # Latest Modify\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=True)(input_layer_2)\n",
    "\n",
    "    embedding_layer = concatenate([embedding_layer, embedding_layer2], axis=2)\n",
    "    embedded_sequences = SpatialDropout1D(0.25)(embedding_layer)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedded_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedded_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedded_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedded_sequences)\n",
    "\n",
    "    attn_0 = AttentionWeightedAverage()(conv_0)\n",
    "    avg_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    \n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    attn_1 = AttentionWeightedAverage()(conv_1)\n",
    "    avg_1 = GlobalAveragePooling1D()(conv_1)    \n",
    "    \n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    attn_2 = AttentionWeightedAverage()(conv_2)\n",
    "    avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    attn_3 = AttentionWeightedAverage()(conv_3)\n",
    "    avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    \n",
    "    \n",
    "    v0_col = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    v1_col = merge([attn_0, attn_1, attn_2, attn_3], mode='concat', concat_axis=1)\n",
    "    v2_col = merge([avg_1, avg_2, avg_0, avg_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor = merge([v0_col, v1_col, v2_col], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.7)(merged_tensor)\n",
    "    output = Dense(units=144)(output)\n",
    "    output = Activation('relu')(output)\n",
    "    #output = Dropout(0.5)(output)\n",
    "    output = Dense(units=6, activation='sigmoid')(output)\n",
    "\n",
    "    model = Model(inputs=[input_layer, input_layer_2], outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the RNN with Attention model structure\n",
    "########################################\n",
    "\n",
    "from keras import optimizers\n",
    "adam_optimizer = optimizers.Adam(lr=1e-3, clipvalue=5, decay=1e-6)\n",
    "\n",
    "def get_av_pos_rnn():\n",
    "    recurrent_units = 56\n",
    "    dropout_rate = 0.35\n",
    "    dense_size = 32\n",
    "    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Onehot')\n",
    "    input_layer_2 = Input(shape=(MAX_SEQUENCE_LENGTH,), name='POS')\n",
    "    \n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)(input_layer)\n",
    "    \n",
    "    embedding_layer2 = Embedding(50,\n",
    "            35, # Latest Modify\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=True)(input_layer_2)\n",
    "\n",
    "    embedding_layer = concatenate([embedding_layer, embedding_layer2], axis=2)\n",
    "    embedding_layer = SpatialDropout1D(0.2)(embedding_layer)\n",
    "    \n",
    "    r1 = Bidirectional(CuDNNGRU(64, return_sequences=True))(embedding_layer)\n",
    "    #r1 = SpatialDropout1D(0.35)(r1) # Latest Modify\n",
    "    #r2 = Bidirectional(CuDNNGRU(64, return_sequences=True))(r1)\n",
    "    #r2 = SpatialDropout1D(0.35)(r2)\n",
    "\n",
    "    #rrs = concatenate([r1 ,r2], axis=-1)\n",
    "    \n",
    "    last_1 = Lambda(lambda t: t[:, -1])(r1)\n",
    "    #last_2 = Lambda(lambda t: t[:, -1])(r2)\n",
    "    maxpool = GlobalMaxPooling1D()(r1)\n",
    "    attn = AttentionWeightedAverage()(r1)\n",
    "    average = GlobalAveragePooling1D()(r1)\n",
    "    \n",
    "    concatenated = concatenate([maxpool, last_1,  attn, average,], axis=1)\n",
    "    x = Dropout(0.5)(concatenated)\n",
    "    x = Dense(144, activation=\"relu\")(x)\n",
    "    output_layer = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[input_layer, input_layer_2], outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam_optimizer,\n",
    "    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models, val_loss, total_auc, fold_predictions = train_folds(data, pos_data, y, 10, 224, get_av_pos_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Overall val-loss:\", val_loss, \"AUC\", total_auc) # RNN benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fold_preditcions = np.concatenate(fold_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_auc = roc_auc_score(y[:-1], train_fold_preditcions)\n",
    "print(\"Training AUC\", training_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_data = test_df\n",
    "CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "submit_path_prefix = \"results/rnn/fasttext-nds-SC-POV-avrnn-Voc\" + str(MAX_NB_WORDS) + \"-fixedpuppet-skippandall-lp-ct-\" + str(MAX_SEQUENCE_LENGTH) \n",
    "\n",
    "print(\"Predicting testing results...\")\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    test_predicts = model.predict({'Onehot':test_data, 'POS':pos_test_data}, batch_size=256, verbose=1)\n",
    "    test_predicts_list.append(test_predicts)\n",
    "    np.save(\"predict_path/\", test_predicts)\n",
    "\n",
    "test_predicts = np.zeros(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts += fold_predict\n",
    "test_predicts /= len(test_predicts_list)\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "submit_path = submit_path_prefix + \"-L{:4f}-A{:4f}.csv\".format(val_loss, total_auc)\n",
    "test_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Predicting training results...\")\n",
    "\n",
    "train_ids = train_df[\"id\"].values\n",
    "train_ids = train_ids.reshape((len(train_ids), 1))\n",
    "\n",
    "train_predicts = pd.DataFrame(data=train_fold_preditcions, columns=CLASSES) # IT MISS THE LAST ONE's label\n",
    "train_predicts[\"id\"] = train_ids[:-1]\n",
    "train_predicts = train_predicts[[\"id\"] + CLASSES]\n",
    "submit_path = submit_path_prefix + \"-Train-L{:4f}-A{:4f}.csv\".format(val_loss, training_auc)\n",
    "train_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
