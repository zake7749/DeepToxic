{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(re.findall(regexp, text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = sorted([f for f in listdir('for_ensemble/') if isfile(join('for_ensemble/', f))])\n",
    "fixed = pd.read_csv('for_ensemble/' + files[0])\n",
    "df = pd.DataFrame()\n",
    "df['id'] = fixed['id']\n",
    "train = pd.read_csv('../Dataset/train.csv')\n",
    "train = train.merge(df, on='id')\n",
    "\n",
    "label = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "test = pd.read_csv('../Dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_index = set()\n",
    "fasttext_index = set()\n",
    "\n",
    "F_EMBEDDING_FILE='../features/crawl-300d-2M.vec'\n",
    "G_EMBEDDING_FILE='../features/glove.840B.300d.txt'\n",
    "\n",
    "f = open(F_EMBEDDING_FILE, 'r', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    try:\n",
    "        fasttext_index.add(values[0])\n",
    "    except:\n",
    "        print(\"Err on \", values[:3])\n",
    "f.close()\n",
    "\n",
    "f = open(G_EMBEDDING_FILE, 'r', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    try:\n",
    "        glove_index.add(values[0])\n",
    "    except:\n",
    "        print(\"Err on \", values[:3])\n",
    "f.close()\n",
    "\n",
    "\n",
    "def count_unknown_glove(t):\n",
    "    t = t.split()\n",
    "    res = 0\n",
    "    for w in t:\n",
    "        if w not in glove_index:\n",
    "            res += 1\n",
    "    return res\n",
    "    \n",
    "def count_unknown_fasttext(t):\n",
    "    t = t.split()\n",
    "    res = 0\n",
    "    for w in t:\n",
    "        if w not in fasttext_index:\n",
    "            res += 1\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_features = pd.read_csv('../features/twitterINFO.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_twitter = twitter_features[twitter_features.type==0]\n",
    "test_twitter= twitter_features[twitter_features.type==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_twitter = train_twitter.drop(['type'], axis=1)\n",
    "test_twitter = test_twitter.drop(['type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(train_twitter, on='id')\n",
    "test = test.merge(test_twitter, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Load the cleaned words\n",
    "########################################\n",
    "\n",
    "cl_path = 'features/cleanwords.txt'\n",
    "clean_word_dict = {}\n",
    "with open(cl_path, 'r', encoding='utf-8') as cl:\n",
    "    for line in cl:\n",
    "        line = line.strip('\\n')\n",
    "        typo, correct = line.split(',')\n",
    "        clean_word_dict[typo] = correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "from collections import defaultdict\n",
    "# Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^?!.,:a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "# regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "word_count_dict = defaultdict(int)\n",
    "import re\n",
    "\n",
    "def clean_text(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    # dirty words\n",
    "    return text\n",
    "    #text = special_character_removal.sub('',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_feature(df):\n",
    "    df['clean_text'] = df['comment_text'].apply(lambda t: clean_text(t))\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                    axis=1)\n",
    "    df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['num_punctuation'] = df['comment_text'].apply(\n",
    "        lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "    df['num_symbols'] = df['comment_text'].apply(\n",
    "        lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "    df['num_words'] = df['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "    df['num_unique_words'] = df['comment_text'].apply(\n",
    "        lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    df['num_smilies'] = df['comment_text'].apply(\n",
    "        lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    # Check number of upper case, if you're angry you may write in upper case\n",
    "    # Number of F words - f..k contains folk, fork,\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    # Number of S word\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    # Number of D words\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    # Number of occurence of You, insulting someone usually needs someone called : you\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    # Just to check you really refered to my mother ;-)\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    # Just checking for toxic 19th century vocabulary\n",
    "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n",
    "    # Some Sentences start with a <:> so it may help\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    # Check for time stamp\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    # Check for dates 18:44, 8 December 2010\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for date short 8 December 2010\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for http links\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    # check for mail\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(\n",
    "        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n",
    "    )\n",
    "    df[\"has_image\"] = df[\"comment_text\"].apply(\n",
    "        lambda x: count_regexp_occ(r'image\\:', x)\n",
    "    )\n",
    "    \n",
    "    df[\"has_ip\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", x))\n",
    "    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "\n",
    "    df[\"has_star\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\*\", x))\n",
    "    df[\"unknown_glove\"] = df['clean_text'].apply(lambda x: count_unknown_glove(x))\n",
    "    df[\"unknown_fasttext\"] = df['clean_text'].apply(lambda x: count_unknown_fasttext(x))\n",
    "    df[\"unknown_glove_fasttext\"] = df[\"unknown_glove\"] + df[\"unknown_fasttext\"]\n",
    "print(\"Creating meta features\")\n",
    "create_meta_feature(train)\n",
    "create_meta_feature(test)\n",
    "print(\"Created.\")\n",
    "\n",
    "train = train.drop(['comment_text'], axis=1)\n",
    "test = test.drop(['comment_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Train Data\")\n",
    "data_path = 'for_ensemble/'\n",
    "train_files = sorted([f for f in listdir(data_path) if isfile(join(data_path, f))])\n",
    "datas= []\n",
    "print(train_files)\n",
    "for file in train_files:\n",
    "    # train_data1 = pd.read_csv(data_path + '/charCNN/kmax11-cnn-1112-1400-Train-L0.044465-A0.982130.csv')\n",
    "    datas.append(pd.read_csv(data_path + file))\n",
    "\n",
    "print(\"Finish Train Data loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(datas):\n",
    "    print(i, data.isnull().sum().sum())\n",
    "    if i == 0:\n",
    "        new_data = data\n",
    "    else : \n",
    "        new_data  = new_data.merge(data, on='id', how='left')\n",
    "        print(i, new_data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below part is for the bug of less predict number. * only do once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_numbers = new_data.shape[1]\n",
    "toxic = new_data.iloc[:, [ i for i in range(1,column_numbers, 6)]] \n",
    "severe_toxic = new_data.iloc[:, [ i for i in range(2,column_numbers, 6)]]\n",
    "obscene = new_data.iloc[:, [ i for i in range(3,column_numbers, 6)]]\n",
    "threat = new_data.iloc[:, [ i for i in range(4,column_numbers, 6)]]\n",
    "insult = new_data.iloc[:, [ i for i in range(5,column_numbers, 6)]]\n",
    "identity_hate = new_data.iloc[:, [ i for i in range(6,column_numbers, 6)]]\n",
    "\n",
    "print(toxic.shape)\n",
    "print(severe_toxic.shape)\n",
    "print(obscene.shape)\n",
    "print(threat.shape)\n",
    "print(insult.shape)\n",
    "print(identity_hate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['total_length', 'capitals', 'caps_vs_length',\n",
    "       'num_exclamation_marks', 'num_question_marks', 'num_punctuation',\n",
    "       'num_symbols', 'num_words', 'num_unique_words', 'words_vs_unique',\n",
    "       'num_smilies', 'ant_slash_n', 'nb_fk', 'nb_sk', 'nb_dk', 'nb_you',\n",
    "       'nb_mother', 'nb_ng', 'start_with_columns', 'has_timestamp',\n",
    "       'has_date_long', 'has_date_short', 'has_http', 'has_mail',\n",
    "       'has_ip', 'has_emphasize_equal', 'has_emphasize_quotes',\n",
    "       'has_star', 'unknown_glove', 'unknown_fasttext',\n",
    "       'unknown_glove_fasttext', 'twitter_prob']:\n",
    "    toxic[col] = train[col]\n",
    "    severe_toxic[col] = train[col]\n",
    "    obscene[col] = train[col]\n",
    "    threat[col] = train[col]\n",
    "    insult[col] = train[col]\n",
    "    identity_hate[col] = train[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function / Load ensemble_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Data\")\n",
    "data_path = 'ensemble_results/'\n",
    "test_datas= []\n",
    "files = sorted([f for f in listdir(data_path) if isfile(join(data_path, f))])\n",
    "print(files)\n",
    "\n",
    "for file in files:\n",
    "    test_datas.append(pd.read_csv(data_path + file))\n",
    "\n",
    "print(\"Finish loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(test_datas):\n",
    "    \n",
    "    print(i, data.isnull().sum().sum())\n",
    "    if i == 0:\n",
    "        new_data = data\n",
    "    else : \n",
    "        new_data  = new_data.merge(data, on='id', how='left')\n",
    "        print(i, new_data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_numbers = new_data.shape[1]\n",
    "test_toxic = new_data.iloc[:, [ i for i in range(1,column_numbers, 6)]]\n",
    "test_severe_toxic = new_data.iloc[:, [ i for i in range(2,column_numbers, 6)]]\n",
    "test_obscene = new_data.iloc[:, [ i for i in range(3,column_numbers, 6)]]\n",
    "test_threat = new_data.iloc[:, [ i for i in range(4,column_numbers, 6)]]\n",
    "test_insult = new_data.iloc[:, [ i for i in range(5,column_numbers, 6)]]\n",
    "test_identity_hate = new_data.iloc[:, [ i for i in range(6,column_numbers, 6)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['total_length', 'capitals', 'caps_vs_length',\n",
    "       'num_exclamation_marks', 'num_question_marks', 'num_punctuation',\n",
    "       'num_symbols', 'num_words', 'num_unique_words', 'words_vs_unique',\n",
    "       'num_smilies', 'ant_slash_n', 'nb_fk', 'nb_sk', 'nb_dk', 'nb_you',\n",
    "       'nb_mother', 'nb_ng', 'start_with_columns', 'has_timestamp',\n",
    "       'has_date_long', 'has_date_short', 'has_http', 'has_mail',\n",
    "       'has_ip', 'has_emphasize_equal', 'has_emphasize_quotes',\n",
    "       'has_star', 'unknown_glove', 'unknown_fasttext',\n",
    "       'unknown_glove_fasttext', 'twitter_prob']:\n",
    "    test_toxic[col] = test[col]\n",
    "    test_severe_toxic[col] = test[col]\n",
    "    test_obscene[col] = test[col]\n",
    "    test_threat[col] = test[col]\n",
    "    test_insult[col] = test[col]\n",
    "    test_identity_hate[col] = test[col]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_datas = [test_toxic, test_severe_toxic, test_obscene, test_threat, test_insult, test_identity_hate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_toxic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_every_feature_model(feature_data, label, feature_name, feature_test_data, predict = False):\n",
    "    predictions = np.zeros(shape=[len(feature_test_data)])\n",
    "    fold_size = len(feature_data) // fold_count\n",
    "\n",
    "    the_label = label[feature_name].values\n",
    "    print(\"Feature name: \", feature_name)\n",
    "    auc = 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        print(\"Fold : \", fold_id)\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(X)\n",
    "        \n",
    "        train_x = np.concatenate([feature_data[:fold_start], feature_data[fold_end:]])\n",
    "        train_y = np.concatenate([the_label[:fold_start], the_label[fold_end:]])\n",
    "\n",
    "        val_x = feature_data[fold_start:fold_end]\n",
    "        val_y = the_label[fold_start:fold_end]\n",
    "        \n",
    "        \n",
    "        lgb_train = lgb.Dataset(train_x, train_y)\n",
    "        lgb_val = lgb.Dataset(val_x, val_y)\n",
    "        \n",
    "        lgbm_model = lgb.LGBMClassifier(max_depth=5, metric=\"auc\", n_estimators=10000,\n",
    "                                   num_leaves=32, boosting_type=\"gbdt\",\n",
    "                                        learning_rate=0.01, feature_fraction=0.3,\n",
    "                                   bagging_fraction=0.8, bagging_freq=5, reg_lambda=0)\n",
    "        lgbm_model.fit(X=train_x, y=train_y,eval_metric=['auc','binary_logloss'],\n",
    "                           eval_set =(val_x, val_y),\n",
    "                          early_stopping_rounds=1000, verbose=500)\n",
    "        auc += lgbm_model.best_score_['valid_0']['auc']\n",
    "        #auc += lgbm_model.best_score_['valid_0']['auc']\n",
    "        lgb.plot_importance(lgbm_model, max_num_features=30)\n",
    "        plt.show()\n",
    "        if predict==True:\n",
    "            prediction = lgbm_model.predict_proba(feature_test_data)\n",
    "            predictions += prediction[:,1]\n",
    "            del lgbm_model\n",
    "    predictions /= fold_count   \n",
    "    print(\"Training  Finish\")\n",
    "\n",
    "    return predictions, auc / fold_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_count = 5\n",
    "all_auc = []\n",
    "predictions = []\n",
    "for i,feature_name in enumerate(label_columns):\n",
    "    prediction, auc = fit_every_feature_model(train_datas[i], label, feature_name, test_datas[i] ,predict = True)\n",
    "    all_auc.append(auc)\n",
    "    predictions.append(prediction)\n",
    "print(\"Overall AUC\", sum(all_auc) / 6)\n",
    "print(\"Each AUC\", all_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training  Finish\n",
    "Overall AUC 0.9918501556540616\n",
    "Each AUC [0.9890730337146506, 0.9920222918771883, 0.9955802803707211, 0.9932634005508755, 0.9901046133930338, 0.9910573140179008]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall AUC 0.991772595736773\n",
    "Each AUC [0.9891636022238185, 0.9922094108617353, 0.9955908009685464, 0.9928002542080963, 0.9902056092164748, 0.9906658969419672]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall AUC 0.992402256021542\n",
    "Each AUC [0.9895523364509066, 0.9924790697348147, 0.9957228439902718, 0.9942676419613532, 0.9903821148935895, 0.9920095290983164]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dart Overall AUC 0.9918463306382743\n",
    "Each AUC [0.9895559477581786, 0.9918782844225191, 0.9956426464701587, 0.9919448445004209, 0.9903768557584822, 0.9916794049198863]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall AUC 0.9924422528049991\n",
    "Each AUC [0.9895998532864534, 0.9925303930007228, 0.9957953188302582, 0.9936798954670287, 0.9905025802713494, 0.9925454759741816]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall AUC 0.9927553975997196\n",
    "Each AUC [0.9896575066913226, 0.9925319108904613, 0.9958337731585984, 0.9951189568519518, 0.9906241195916726, 0.9927661184143105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall AUC 0.9928417266022279\n",
    "Each AUC [0.9898409916016264, 0.9926231321876025, 0.9959125410608889, 0.9950726242194354, 0.9907445504706871, 0.9928565200731265]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall AUC 0.9927522465565181\n",
    "Each AUC [0.9898409027947984, 0.9926108135621181, 0.9959050092189476, 0.9944645254713077, 0.9907610116240857, 0.99293121666785]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall AUC 0.9928923711203441\n",
    "Each AUC [0.98983781123029, 0.9926462987999607, 0.9959119831639403, 0.9952592506885705, 0.9907596664543711, 0.9929392163849318]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall AUC 0.991030175791376\n",
    "Each AUC [0.9878416437898958, 0.990220606684348, 0.9953303378054033, 0.9924730357089938, 0.9896683886147624, 0.990647042144853]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall AUC 0.9926881408536721\n",
    "Each AUC [0.9898203113911324, 0.992560373593184, 0.9958975654820794, 0.9944830495431566, 0.9907448846625799, 0.9926226604499003]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm  = pd.read_csv('Dataset/sample_submission.csv')\n",
    "for i,feature_name in enumerate(label_columns):\n",
    "    subm[feature_name] = predictions[i]\n",
    "subm.to_csv('LGBM_RANDR_99289_F5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
